{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTS-T100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.3.1`\n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://0a3a93d19243:4041\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@20ea7616"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = {\n",
    "  NotebookSparkSession\n",
    "    .builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sc = spark\n",
    "    .sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create an `RDD` and run some calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc\n",
    "    .parallelize(1 to 100000000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val n = rdd\n",
    "    .map(_ + 1)\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute a Spark action like `sum` you should see a progress bar, showing the progress of the running Spark job. If you're using the Jupyter classic UI, you can also click on *(kill)* to cancel the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val n = rdd\n",
    "    .map(n => \n",
    "        (n % 10, n)\n",
    "    )\n",
    "    .reduceByKey(_ + _)\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syncing Dependencies\n",
    "\n",
    "If extra dependencies are loaded, via ``import $ivy.`â€¦` `` after the `SparkSession` has been created, you should call `NotebookSparkSession.sync()` for the newly added JARs to be passed to the Spark executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.typelevel::cats-core:1.6.0`\n",
    "\n",
    "NotebookSparkSession.sync() // cats should be available on workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataframes\n",
    "\n",
    "If you try to create a `Dataset` or a `Dataframe` from some data structure containing a case class and you're getting an `org.apache.spark.sql.AnalysisException: Unable to generate an encoder for inner class ...` when calling `.toDS`/`.toDF`, try the following workaround:\n",
    "\n",
    "Add `org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)` in the same cell where you define case classes involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this);\n",
    "\n",
    "case class Person(id: String, value: Int)\n",
    "\n",
    "val ds = List(\n",
    "    Person(\"Alice\", 42), \n",
    "    Person(\"Bob\", 43), \n",
    "    Person(\"Charlie\", 44)\n",
    ").toDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workaround won't be neccessary anymore in future Spark versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rich Display of Datasets and Dataframes\n",
    "\n",
    "As of now, *almond-spark* doesn't include native rich display capabilities for Datasets and Dataframes. So by default, we only have ascii rendering of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too hard to add your own displayer though. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// based on a snippet by Ivan Zaitsev\n",
    "// https://github.com/almond-sh/almond/issues/180#issuecomment-364711999\n",
    "implicit class RichDF(val df: DataFrame) {\n",
    "  def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
    "    import xml.Utility.escape\n",
    "    val data = df.take(limit)\n",
    "    val header = df.schema.fieldNames.toSeq\n",
    "    val rows: Seq[Seq[String]] = data.map { row =>\n",
    "      row.toSeq.map { cell =>\n",
    "        val str = cell match {\n",
    "          case null => \"null\"\n",
    "          case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "          case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "          case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "          case _ => cell.toString\n",
    "        }\n",
    "        if (truncate > 0 && str.length > truncate) {\n",
    "          // do not show ellipses for strings shorter than 4 characters.\n",
    "          if (truncate < 4) str.substring(0, truncate)\n",
    "          else str.substring(0, truncate - 3) + \"...\"\n",
    "        } else {\n",
    "          str\n",
    "        }\n",
    "      }: Seq[String]\n",
    "    }\n",
    "\n",
    "    publish.html(s\"\"\"\n",
    "      <table class=\"table\">\n",
    "        <tr>\n",
    "        ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "        </tr>\n",
    "        ${rows.map { row =>\n",
    "          s\"<tr>${row.map { c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "        }.mkString\n",
    "        }\n",
    "      </table>\"\"\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.toDF.showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
